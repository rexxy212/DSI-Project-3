{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53993281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries used\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323fd84f",
   "metadata": {},
   "source": [
    "# Data Collection with Pushshift API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a69cbd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pushshift api\n",
    "url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a88321d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data collection for The Onion subreddit\n",
    "\n",
    "#creating empty dataframe to house data scrapped\n",
    "onion_df = pd.DataFrame() \n",
    "\n",
    "#parameters for pushshift api, epoch value of 1635120000 is 25/10/2021 00:00\n",
    "params_onion = {\"subreddit\" : \"theonion\",\n",
    "                  \"fields\": ['subreddit', 'title', 'removed_by_category', 'created_utc'],\n",
    "                  \"size\": 100,\n",
    "                  \"before\": 1635120000}\n",
    "\n",
    "#loop to collect at least 2000 data points as pushshift is limited to 100 posts per run\n",
    "while len(onion_df) < 2000:\n",
    "    onion_res = requests.get(url, params_onion)\n",
    "    \n",
    "    #checks if code is good to go\n",
    "    if onion_res.status_code == 200:\n",
    "        onion_add_df = pd.DataFrame(onion_res.json()['data'])\n",
    "        \n",
    "        #checks and removes deleted posts\n",
    "        onion_df = onion_df.append(onion_add_df[onion_add_df['removed_by_category'].isnull()])\n",
    "        \n",
    "        #checks and removes duplicate posts\n",
    "        onion_df.drop_duplicates(subset=['title'],inplace=True)\n",
    "        \n",
    "        #updates parameters's date to last post's\n",
    "        params_onion.update({\"before\":(onion_df[\"created_utc\"].iloc[-1])})\n",
    "        \n",
    "#if api is overloaded, i.e, res status code = 493, rests for 1 second before re-attempting collection        \n",
    "    else:  \n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1eca6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data collection for News subreddit\n",
    "\n",
    "#creating empty dataframe to house data scrapped\n",
    "news_df = pd.DataFrame()\n",
    "\n",
    "#parameters for pushshift api, epoch value of 1635120000 is 25/10/2021 00:00\n",
    "params_news = {\"subreddit\" : \"news\",\n",
    "                  \"fields\": ['subreddit', 'title', 'removed_by_category', 'created_utc',''],\n",
    "                  \"size\": 100,\n",
    "                  \"before\": 1635120000}\n",
    "\n",
    "#loop to collect at least 2000 data points as pushshift is limited to 100 posts per run\n",
    "while len(news_df) < 2000:\n",
    "    news_res = requests.get(url, params_news)\n",
    "    \n",
    "    #checks if code is good to go\n",
    "    if news_res.status_code == 200:\n",
    "        news_add_df = pd.DataFrame(news_res.json()['data'])\n",
    "        \n",
    "        #checks and removes for deleted posts\n",
    "        news_df = news_df.append(news_add_df[news_add_df['removed_by_category'].isnull()])\n",
    "        \n",
    "        #checks and removes duplicate posts\n",
    "        news_df.drop_duplicates(subset=['title'],inplace=True)\n",
    "        \n",
    "        #updates parameters's date to last post's\n",
    "        params_news.update({\"before\":(news_df[\"created_utc\"].iloc[-1])})\n",
    "        \n",
    "#if api is overloaded, i.e, res status code = 493, rests for 1 second before re-attempting collection \n",
    "    else: \n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5552a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of The Onion title: 2018\n",
      "No. of News Title: 2011\n"
     ]
    }
   ],
   "source": [
    "#check no. of posts scrapped \n",
    "print(\"No. of The Onion title: \" + str(len(onion_df)))\n",
    "print(\"No. of News Title: \" + str(len(news_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037ef13c",
   "metadata": {},
   "source": [
    "# Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fc755b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Onion duplicates: 0\n",
      "News duplicates: 0\n",
      "\n",
      "\n",
      "No. of null entries for The Onion: \n",
      "created_utc               0\n",
      "subreddit                 0\n",
      "title                     0\n",
      "removed_by_category    2018\n",
      "dtype: int64\n",
      "No. of null entries for News: \n",
      "created_utc               0\n",
      "removed_by_category    2011\n",
      "subreddit                 0\n",
      "title                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check for duplicate posts\n",
    "print(\"The Onion duplicates: \" + str(onion_df[['title']].duplicated().sum()))\n",
    "print(\"News duplicates: \" + str(news_df[['title']].duplicated().sum()))\n",
    "print(\"\\n\")\n",
    " \n",
    "#check for null entries\n",
    "print(\"No. of null entries for The Onion: \" + \"\\n\"  + str(onion_df.isnull().sum()))\n",
    "print(\"No. of null entries for News: \" + \"\\n\" + str(news_df.isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3419a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.info of     created_utc subreddit                                              title  \\\n",
      "0    1635090051  TheOnion  Smithsonian Acquires Arms Of Kermit The Frog P...   \n",
      "1    1634958726  TheOnion  Less Popular Friend Only Included In Suicide P...   \n",
      "3    1634940672  TheOnion  Crypto-Averse Man Would Prefer Investing In Tr...   \n",
      "4    1634910709  TheOnion  God Loses Pouch Filled With Crystals That Give...   \n",
      "7    1634872463  TheOnion  Retired NFL Player Touts Sports Betting App As...   \n",
      "..          ...       ...                                                ...   \n",
      "95   1580708304  TheOnion  The Onion is made a podcast called \"The Topica...   \n",
      "96   1580691176  TheOnion  ‘I’m Just Here For The Commercials,’ Jokes Man...   \n",
      "97   1580669936  TheOnion     My god how much Budget did they have for this?   \n",
      "98   1580662117  TheOnion      Snacks Distract Lawmakers From Horrors of War   \n",
      "99   1580589888  TheOnion  Onion Talks: Hypothetically It Would Be Okay T...   \n",
      "\n",
      "   removed_by_category  \n",
      "0                  NaN  \n",
      "1                  NaN  \n",
      "3                  NaN  \n",
      "4                  NaN  \n",
      "7                  NaN  \n",
      "..                 ...  \n",
      "95                 NaN  \n",
      "96                 NaN  \n",
      "97                 NaN  \n",
      "98                 NaN  \n",
      "99                 NaN  \n",
      "\n",
      "[2018 rows x 4 columns]>\n",
      "<bound method DataFrame.info of     created_utc removed_by_category subreddit  \\\n",
      "4    1635118883                 NaN      news   \n",
      "7    1635118006                 NaN      news   \n",
      "15   1635116237                 NaN      news   \n",
      "23   1635115094                 NaN      news   \n",
      "25   1635114451                 NaN      news   \n",
      "..          ...                 ...       ...   \n",
      "82   1633613759                 NaN      news   \n",
      "83   1633613721                 NaN      news   \n",
      "84   1633613692                 NaN      news   \n",
      "85   1633613650                 NaN      news   \n",
      "99   1633612973                 NaN      news   \n",
      "\n",
      "                                                title  \n",
      "4   Supporters of unvaccinated NBA star Kyrie Irvi...  \n",
      "7   Body of missing 26-year-old Texas man found in...  \n",
      "15  Two SF Prosecutors Quit &amp; Join Effort to O...  \n",
      "23  James Michael Tyler, who played Gunther in 'Fr...  \n",
      "25  Plane from NY to LA makes emergency landing in...  \n",
      "..                                                ...  \n",
      "82  Man laundered $1M from romance scheme that tar...  \n",
      "83  Florida lifts 30-year ban on catching goliath ...  \n",
      "84  After Keys deputies talk to injured woman, boy...  \n",
      "85  Dutch watchdog finds Apple app store payment r...  \n",
      "99  Riverside County Sheriff Chad Bianco acknowled...  \n",
      "\n",
      "[2011 rows x 4 columns]>\n"
     ]
    }
   ],
   "source": [
    "#preliminary look at data\n",
    "print(onion_df.info)\n",
    "print(news_df.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6640eeb2",
   "metadata": {},
   "source": [
    "Index seems to be messed up, will reset index. Since 'remove_by_category' already helped sifted deleted posts, it is no longer needed and column can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d662c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index for both data sets\n",
    "onion_df.reset_index(inplace=True)\n",
    "news_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81f048e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of old index and removed_by_category columns\n",
    "onion_df.drop([\"index\", \"removed_by_category\"], axis=1, inplace=True)\n",
    "news_df.drop([\"index\", \"removed_by_category\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4506c927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['created_utc', 'subreddit', 'title'], dtype='object')\n",
      "Index(['created_utc', 'subreddit', 'title'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#check if columns were dropped\n",
    "print(onion_df.columns)\n",
    "print(news_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8deb562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting collected data\n",
    "onion_df.to_csv('theonion', index=False)\n",
    "news_df.to_csv('news', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
